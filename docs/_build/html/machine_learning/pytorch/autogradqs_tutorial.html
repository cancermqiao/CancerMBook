
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>5. Automatic Differentiation with Torch.autograd &#8212; CancerM Book</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.e8f53015daec13862f6db5e763c41738.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/tabs.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"TeX": {"Macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://jupyterbook.org/machine_learning/pytorch/autogradqs_tutorial.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="6. Optimizing Model Parameters" href="optimization_tutorial.html" />
    <link rel="prev" title="4. Build the Neural Network" href="buildmodel_tutorial.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
      
      <h1 class="site-logo" id="site-title">CancerM Book</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p class="caption">
 <span class="caption-text">
  机器学习(Machine Learning)
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Machine%20Learning%202021%28%E6%9D%8E%E5%AE%8F%E6%AF%85%29/overview.html">
   Machine Learning 2021(李宏毅)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Machine%20Learning%202021%28%E6%9D%8E%E5%AE%8F%E6%AF%85%29/ML2021Spring_HW1.html">
     Homework 1: COVID-19 Cases Prediction (Regression)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Machine%20Learning%202021%28%E6%9D%8E%E5%AE%8F%E6%AF%85%29/Active%20Function.html">
     常用激活函数
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="intro.html">
   Pytorch Tutorial
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="quickstart_tutorial.html">
     0. Quickstart
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="tensorqs_tutorial.html">
     1. Tensors
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="data_tutorial.html">
     2. Datasets &amp; DataLoaders
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="transforms_tutorial.html">
     3. Transforms
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="buildmodel_tutorial.html">
     4. Build the Neural Network
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     5. Automatic Differentiation with
     <code class="docutils literal notranslate">
      <span class="pre">
       Torch.autograd
      </span>
     </code>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_tutorial.html">
     6. Optimizing Model Parameters
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="saveloadrun_tutorial.html">
     7. Save and Load the Model
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  强化学习(Reinforcement Learning)
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../reinforcement_learning/ucl_course_on_rl_by_David_Silver/overview.html">
   UCL Course on RL
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../reinforcement_learning/ucl_course_on_rl_by_David_Silver/introduction.html">
     Lecture 1: Introduction to Reinforcement Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../reinforcement_learning/ucl_course_on_rl_by_David_Silver/mdp.html">
     Lecture 2: Markov Decision Processes
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  因果推断(Causal Inference)
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../causal_inference/introduction_to_causal_inference/overview.html">
   Introduction to Causal Inference
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../causal_inference/introduction_to_causal_inference/ch1.html">
     1. Motivation: Why You Might Care
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../causal_inference/introduction_to_causal_inference/ch2.html">
     2 Potential Outcomes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../causal_inference/introduction_to_causal_inference/ch3.html">
     3. The Flow of Association and Causation in Graphs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../causal_inference/introduction_to_causal_inference/ch4.html">
     4. Causal Models
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../causal_inference/causal_inference_and_learning/overview.html">
   Causal Inference and Learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../causal_inference/causal_inference_and_learning/computational_and_thinking.html">
     0. Computational and Inferential thinking
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  编程(Program)
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../program/%E5%89%91%E6%8C%87offer/overview.html">
   剑指offer
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../program/%E5%89%91%E6%8C%87offer/%E6%95%B0%E7%BB%84%E5%88%86%E5%89%B2.html">
     数组分割
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../program/%E5%89%91%E6%8C%87offer/%E7%AC%ACk%E5%A4%A7%E7%9A%84%E6%95%B0.html">
     第K大的数
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../program/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/overview.html">
   排序算法
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../program/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/sort.html">
     快速排序
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  已读文章(Paper Read)
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../papers/reinforcement_learning/overview.html">
   Here’s my sample title
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../papers/reinforcement_learning/learning_MDPs_from_features.html">
     Here’s my sample title
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../papers/reinforcement_learning/notebooks.html">
     Jupyter Notebook files
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/machine_learning/pytorch/autogradqs_tutorial.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/cancermqiao/CancerMBook.git"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/cancermqiao/CancerMBook.git/issues/new?title=Issue%20on%20page%20%2Fmachine_learning/pytorch/autogradqs_tutorial.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/cancermqiao/CancerMBook.git/edit/master/docs/machine_learning/pytorch/autogradqs_tutorial.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/cancermqiao/CancerMBook.git/master?urlpath=tree/docs/machine_learning/pytorch/autogradqs_tutorial.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/cancermqiao/CancerMBook.git/blob/master/docs/machine_learning/pytorch/autogradqs_tutorial.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tensors-functions-and-computational-graph">
   Tensors, Functions and Computational graph
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#computing-gradients">
   Computing Gradients
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#disabling-gradient-tracking">
   Disabling Gradient Tracking
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#more-on-computational-graphs">
   More on Computational Graphs
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optional-reading-tensor-gradients-and-jacobian-products">
   Optional Reading: Tensor Gradients and Jacobian Products
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#further-reading">
   Further Reading
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="automatic-differentiation-with-torch-autograd">
<h1>5. Automatic Differentiation with <code class="docutils literal notranslate"><span class="pre">Torch.autograd</span></code><a class="headerlink" href="#automatic-differentiation-with-torch-autograd" title="Permalink to this headline">¶</a></h1>
<p>When training neural networks, the most frequently used algorithm is
<strong>back propagation</strong>. In this algorithm, parameters (model weights) are
adjusted according to the <strong>gradient</strong> of the loss function with respect
to the given parameter.</p>
<p>To compute those gradients, PyTorch has a built-in differentiation engine
called <code class="docutils literal notranslate"><span class="pre">torch.autograd</span></code>. It supports automatic computation of gradient for any
computational graph.</p>
<p>Consider the simplest one-layer neural network, with input <code class="docutils literal notranslate"><span class="pre">x</span></code>,
parameters <code class="docutils literal notranslate"><span class="pre">w</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code>, and some loss function. It can be defined in
PyTorch in the following manner:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>  <span class="c1"># input tensor</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>  <span class="c1"># expected output</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span><span class="o">+</span><span class="n">b</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="tensors-functions-and-computational-graph">
<h2>Tensors, Functions and Computational graph<a class="headerlink" href="#tensors-functions-and-computational-graph" title="Permalink to this headline">¶</a></h2>
<p>This code defines the following <strong>computational graph</strong>:</p>
<a class="reference internal image-reference" href="../../_images/comp-graph.png"><img alt="../../_images/comp-graph.png" src="../../_images/comp-graph.png" style="width: 700px;" /></a>
<p>In this network, <code class="docutils literal notranslate"><span class="pre">w</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code> are <strong>parameters</strong>, which we need to
optimize. Thus, we need to be able to compute the gradients of loss
function with respect to those variables. In order to do that, we set
the <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> property of those tensors.</p>
<div class="note admonition">
<p class="admonition-title">NOTE</p>
<p>You can set the value of <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> when creating a tensor, or later by using <code class="docutils literal notranslate"><span class="pre">x.requires_grad_(True)</span></code> method.</p>
</div>
<p>A function that we apply to tensors to construct computational graph is
in fact an object of class <code class="docutils literal notranslate"><span class="pre">Function</span></code>. This object knows how to
compute the function in the <em>forward</em> direction, and also how to compute
its derivative during the <em>backward propagation</em> step. A reference to
the backward propagation function is stored in <code class="docutils literal notranslate"><span class="pre">grad_fn</span></code> property of a
tensor. You can find more information of <code class="docutils literal notranslate"><span class="pre">Function</span></code> <a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#function">in the
documentation</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Gradient function for z =&#39;</span><span class="p">,</span> <span class="n">z</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Gradient function for loss =&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Gradient function for z = &lt;AddBackward0 object at 0x7ff7fd77e940&gt;
Gradient function for loss = &lt;BinaryCrossEntropyWithLogitsBackward object at 0x7ff7fd77efa0&gt;
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="computing-gradients">
<h2>Computing Gradients<a class="headerlink" href="#computing-gradients" title="Permalink to this headline">¶</a></h2>
<p>To optimize weights of parameters in the neural network, we need to
compute the derivatives of our loss function with respect to parameters,
namely, we need <span class="math notranslate nohighlight">\(\frac{\partial loss}{\partial w}\)</span> and
<span class="math notranslate nohighlight">\(\frac{\partial loss}{\partial b}\)</span> under some fixed values of
<code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code>. To compute those derivatives, we call
<code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code>, and then retrieve the values from <code class="docutils literal notranslate"><span class="pre">w.grad</span></code> and
<code class="docutils literal notranslate"><span class="pre">b.grad</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[0.0159, 0.2858, 0.0136],
        [0.0159, 0.2858, 0.0136],
        [0.0159, 0.2858, 0.0136],
        [0.0159, 0.2858, 0.0136],
        [0.0159, 0.2858, 0.0136]])
tensor([0.0159, 0.2858, 0.0136])
</pre></div>
</div>
</div>
</div>
<div class="note admonition">
<p class="admonition-title">NOTE</p>
<ul class="simple">
<li><p>We can only obtain the <code class="docutils literal notranslate"><span class="pre">grad</span></code> properties for the leaf
nodes of the computational graph, which have <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> property
set to <code class="docutils literal notranslate"><span class="pre">True</span></code>. For all other nodes in our graph, gradients will not be
available.</p></li>
<li><p>We can only perform gradient calculations using
<code class="docutils literal notranslate"><span class="pre">backward</span></code> once on a given graph, for performance reasons. If we need
to do several <code class="docutils literal notranslate"><span class="pre">backward</span></code> calls on the same graph, we need to pass
<code class="docutils literal notranslate"><span class="pre">retain_graph=True</span></code> to the <code class="docutils literal notranslate"><span class="pre">backward</span></code> call.</p></li>
</ul>
</div>
</div>
<div class="section" id="disabling-gradient-tracking">
<h2>Disabling Gradient Tracking<a class="headerlink" href="#disabling-gradient-tracking" title="Permalink to this headline">¶</a></h2>
<p>By default, all tensors with <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code> are tracking their
computational history and support gradient computation. However, there
are some cases when we do not need to do that, for example, when we have
trained the model and just want to apply it to some input data, i.e. we
only want to do <em>forward</em> computations through the network. We can stop
tracking computations by surrounding our computation code with
<code class="docutils literal notranslate"><span class="pre">torch.no_grad()</span></code> block:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span><span class="o">+</span><span class="n">b</span>
<span class="nb">print</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span><span class="o">+</span><span class="n">b</span>
<span class="nb">print</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
False
</pre></div>
</div>
</div>
</div>
<p>Another way to achieve the same result is to use the <code class="docutils literal notranslate"><span class="pre">detach()</span></code> method
on the tensor:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span><span class="o">+</span><span class="n">b</span>
<span class="n">z_det</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">z_det</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>False
</pre></div>
</div>
</div>
</div>
<p>There are reasons you might want to disable gradient tracking:</p>
<ul class="simple">
<li><p>To mark some parameters in your neural network as <strong>frozen parameters</strong>. This is
a very common scenario for
<a class="reference external" href="https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html">finetuning a pretrained network</a></p></li>
<li><p>To <strong>speed up computations</strong> when you are only doing forward pass, because computations on tensors that do
not track gradients would be more efficient.</p></li>
</ul>
</div>
<div class="section" id="more-on-computational-graphs">
<h2>More on Computational Graphs<a class="headerlink" href="#more-on-computational-graphs" title="Permalink to this headline">¶</a></h2>
<p>Conceptually, autograd keeps a record of data (tensors) and all executed
operations (along with the resulting new tensors) in a directed acyclic
graph (DAG) consisting of
<a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function">Function</a>
objects. In this DAG, leaves are the input tensors, roots are the output
tensors. By tracing this graph from roots to leaves, you can
automatically compute the gradients using the chain rule.</p>
<p>In a forward pass, autograd does two things simultaneously:</p>
<ul class="simple">
<li><p>run the requested operation to compute a resulting tensor</p></li>
<li><p>maintain the operation’s <em>gradient function</em> in the DAG.</p></li>
</ul>
<p>The backward pass kicks off when <code class="docutils literal notranslate"><span class="pre">.backward()</span></code> is called on the DAG
root. <code class="docutils literal notranslate"><span class="pre">autograd</span></code> then:</p>
<ul class="simple">
<li><p>computes the gradients from each <code class="docutils literal notranslate"><span class="pre">.grad_fn</span></code>,</p></li>
<li><p>accumulates them in the respective tensor’s <code class="docutils literal notranslate"><span class="pre">.grad</span></code> attribute</p></li>
<li><p>using the chain rule, propagates all the way to the leaf tensors.</p></li>
</ul>
<div class="note admonition">
<p class="admonition-title">NOTE</p>
<p><strong>DAGs are dynamic in PyTorch</strong>
An important thing to note is that the graph is recreated from scratch; after each
<code class="docutils literal notranslate"><span class="pre">.backward()</span></code> call, autograd starts populating a new graph. This is
exactly what allows you to use control flow statements in your model;
you can change the shape, size and operations at every iteration if
needed.</p>
</div>
</div>
<div class="section" id="optional-reading-tensor-gradients-and-jacobian-products">
<h2>Optional Reading: Tensor Gradients and Jacobian Products<a class="headerlink" href="#optional-reading-tensor-gradients-and-jacobian-products" title="Permalink to this headline">¶</a></h2>
<p>In many cases, we have a scalar loss function, and we need to compute
the gradient with respect to some parameters. However, there are cases
when the output function is an arbitrary tensor. In this case, PyTorch
allows you to compute so-called <strong>Jacobian product</strong>, and not the actual
gradient.</p>
<p>For a vector function <span class="math notranslate nohighlight">\(\vec{y}=f(\vec{x})\)</span>, where
<span class="math notranslate nohighlight">\(\vec{x}=\langle x_1,\dots,x_n\rangle\)</span> and
<span class="math notranslate nohighlight">\(\vec{y}=\langle y_1,\dots,y_m\rangle\)</span>, a gradient of
<span class="math notranslate nohighlight">\(\vec{y}\)</span> with respect to <span class="math notranslate nohighlight">\(\vec{x}\)</span> is given by <strong>Jacobian
matrix</strong>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-8d741d94-5274-44f0-9871-61905a69bc5a">
<span class="eqno">(13)<a class="headerlink" href="#equation-8d741d94-5274-44f0-9871-61905a69bc5a" title="Permalink to this equation">¶</a></span>\[\begin{align}J=\left(\begin{array}{ccc}
      \frac{\partial y_{1}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{1}}{\partial x_{n}}\\
      \vdots &amp; \ddots &amp; \vdots\\
      \frac{\partial y_{m}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{m}}{\partial x_{n}}
      \end{array}\right)\end{align}\]</div>
<p>Instead of computing the Jacobian matrix itself, PyTorch allows you to
compute <strong>Jacobian Product</strong> <span class="math notranslate nohighlight">\(v^T\cdot J\)</span> for a given input vector
<span class="math notranslate nohighlight">\(v=(v_1 \dots v_m)\)</span>. This is achieved by calling <code class="docutils literal notranslate"><span class="pre">backward</span></code> with
<span class="math notranslate nohighlight">\(v\)</span> as an argument. The size of <span class="math notranslate nohighlight">\(v\)</span> should be the same as
the size of the original tensor, with respect to which we want to
compute the product:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">inp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">inp</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">out</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">inp</span><span class="p">),</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;First call</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
<span class="n">out</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">inp</span><span class="p">),</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Second call</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
<span class="n">inp</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
<span class="n">out</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">inp</span><span class="p">),</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Call after zeroing gradients</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">inp</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>First call
 tensor([[4., 2., 2., 2., 2.],
        [2., 4., 2., 2., 2.],
        [2., 2., 4., 2., 2.],
        [2., 2., 2., 4., 2.],
        [2., 2., 2., 2., 4.]])

Second call
 tensor([[8., 4., 4., 4., 4.],
        [4., 8., 4., 4., 4.],
        [4., 4., 8., 4., 4.],
        [4., 4., 4., 8., 4.],
        [4., 4., 4., 4., 8.]])

Call after zeroing gradients
 tensor([[4., 2., 2., 2., 2.],
        [2., 4., 2., 2., 2.],
        [2., 2., 4., 2., 2.],
        [2., 2., 2., 4., 2.],
        [2., 2., 2., 2., 4.]])
</pre></div>
</div>
</div>
</div>
<p>Notice that when we call <code class="docutils literal notranslate"><span class="pre">backward</span></code> for the second time with the same
argument, the value of the gradient is different. This happens because
when doing <code class="docutils literal notranslate"><span class="pre">backward</span></code> propagation, PyTorch <strong>accumulates the
gradients</strong>, i.e. the value of computed gradients is added to the
<code class="docutils literal notranslate"><span class="pre">grad</span></code> property of all leaf nodes of computational graph. If you want
to compute the proper gradients, you need to zero out the <code class="docutils literal notranslate"><span class="pre">grad</span></code>
property before. In real-life training an <em>optimizer</em> helps us to do
this.</p>
<div class="note admonition">
<p class="admonition-title">NOTE</p>
<p>Previously we were calling <code class="docutils literal notranslate"><span class="pre">backward()</span></code> function without
parameters. This is essentially equivalent to calling
<code class="docutils literal notranslate"><span class="pre">backward(torch.tensor(1.0))</span></code>, which is a useful way to compute the
gradients in case of a scalar-valued function, such as loss during
neural network training.</p>
</div>
</div>
<hr class="docutils" />
<div class="section" id="further-reading">
<h2>Further Reading<a class="headerlink" href="#further-reading" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/notes/autograd.html">Autograd Mechanics</a></p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "cancermqiao/CancerMBook.git",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./machine_learning/pytorch"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="buildmodel_tutorial.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">4. Build the Neural Network</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="optimization_tutorial.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">6. Optimizing Model Parameters</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By CancerM Qiao<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-52617120-7', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </body>
</html>